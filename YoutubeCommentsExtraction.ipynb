{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "474ed180-436f-4587-8271-693898cc951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste the LINK of the YouTube video Here:  https://www.youtube.com/watch?v=JeInwySxuxA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments extracted from Video Successfully\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_video_id(url):\n",
    "    # Regular expression to match the video ID\n",
    "    pattern = r\"(?<=v=)[a-zA-Z0-9_-]+(?=&|\\s|$)\"\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Get Video link from the user\n",
    "user_input = input(\"Paste the LINK of the YouTube video Here: \")\n",
    "\n",
    "# Extract video ID from user input\n",
    "video_id = extract_video_id(user_input)\n",
    "\n",
    "if video_id:\n",
    "        print(\"Comments extracted from Video Successfully\")\n",
    "else:\n",
    "    print(\"Invalid YouTube video link.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f1128c08-b08b-40a6-81c7-d3a90e07b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "\n",
    "comments_count=2000\n",
    "def extract_comments(video_id, api_key):\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    \n",
    "    youtube = googleapiclient.discovery.build (\n",
    "        api_service_name, api_version, developerKey=api_key)\n",
    "\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    while len(comments) < comments_count:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=min(comments_count - len(comments), 100),\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append([\n",
    "                comment['authorDisplayName'],\n",
    "                comment['publishedAt'],\n",
    "                comment['updatedAt'],\n",
    "                comment['likeCount'],\n",
    "                comment['textDisplay']\n",
    "            ])\n",
    "        \n",
    "        if 'nextPageToken' in response:\n",
    "            next_page_token = response['nextPageToken']\n",
    "        else:\n",
    "            break  # No more comments\n",
    "        \n",
    "    return comments\n",
    "\n",
    "api_key = \"AIzaSyC2LsktWN6n-W9jzFGGNEGqRCbGgvaXmmc\"\n",
    "\n",
    "# Extract comments from the video\n",
    "comments = extract_comments(video_id, api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f490e4ad-9344-4de9-964f-7b9c6fcfdcb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing the Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(comments, columns = ['author', 'publised_at', 'updated_at', 'like_count', 'text'])\n",
    "file_path = '/Users/user/Desktop/NLP Projects/comments.csv'\n",
    "df.to_csv(file_path,index = False)\n",
    "\n",
    "messages = pd.read_csv('/Users/user/Desktop/NLP Projects/comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "57aa1c9a-9075-4de5-af8c-20bd023d9fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Define a function to get sentiment\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    if blob.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif blob.sentiment.polarity < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the sentiment analysis function to each comment and create a new column\n",
    "messages['Sentiment'] = df['text'].apply(get_sentiment)\n",
    "\n",
    "# Save the DataFrame with the new column to a new CSV file\n",
    "messages.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7cbcb19b-6a61-4b20-8291-88aaae0bd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of positive, negative, neutral comments\n",
    "positive_comments = 0\n",
    "negative_comments = 0\n",
    "neutral_comments = 0\n",
    "for index, row in messages.iterrows():\n",
    "    sentiment = get_sentiment(row['text'])\n",
    "    if sentiment == 1:\n",
    "        positive_comments += 1\n",
    "    elif sentiment == -1:\n",
    "        negative_comments += 1\n",
    "    else:\n",
    "        neutral_comments += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9fef1935-1533-41e4-ac75-1fc016b302df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning and preprocessing\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "w = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(0, len(messages['text'])):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [w.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "49bb57ea-c96b-4131-8fa2-df700b9c24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=2525)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "y=pd.get_dummies(messages['Sentiment'])\n",
    "y=y.iloc[:,1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "86ee4587-b127-465c-bb1a-f40c16e264d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Training model using Naive bayes classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "comment_review_model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_pred=comment_review_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d7631d09-3c90-4d39-acbb-ff2073bf98cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[215,  33],\n",
       "       [ 67,  85]], dtype=int64)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_m = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "confusion_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "162ce3b1-b37f-46ec-bd00-bef0a8fa474d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "d00d32c3-0091-4df9-8056-2b43983c71c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.0 % liked the video\n",
      "17.4 % doesn't liked the video\n",
      "38.6 % are neutral\n",
      "Machine Learning Model Accuracy: 75.0 %\n"
     ]
    }
   ],
   "source": [
    "positive_percentage = positive_comments/comments_count\n",
    "print(f\"{positive_percentage*100} % liked the video\")\n",
    "\n",
    "negative_percentage = negative_comments/comments_count\n",
    "print(f\"{negative_percentage*100} % doesn't liked the video\")\n",
    "\n",
    "neutral_percentage = neutral_comments/comments_count\n",
    "print(f\"{neutral_percentage*100} % are neutral\")\n",
    "\n",
    "print(f\"Machine Learning Model Accuracy: {accuracy*100} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
